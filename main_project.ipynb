{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a12009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.keras import regularizers\n",
    "from keras import optimizers, metrics, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.layers import Dense, Conv2D, Flatten, Activation, MaxPooling2D, Dropout,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset \n",
    "data_dir=\"./leaf_diseases\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the batch_size and image_size\n",
    "BATCH_SIZE=32\n",
    "IMAGE_SIZE=256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0470ab40",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training(80%) and testing(20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d74993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the samples\n",
    "plt.figure(figsize=(10, 10))\n",
    "for image_batch, labels_batch in train_ds.take(1):\n",
    "    for i in range(12):\n",
    "        ax = plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels_batch[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3564e",
   "metadata": {},
   "source": [
    "## CNN algorithm is used by doing some variations to improve accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271d45c",
   "metadata": {},
   "source": [
    "### Initial model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efae980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-initial model\n",
    "num_classes = 8\n",
    "model = Sequential([\n",
    "  layers.Dense(10, activation='relu'),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(num_classes)\n",
    "])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-initial model training\n",
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=test_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de552a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc=model.evaluate(test_ds,verbose=2)\n",
    "print('\\nTest accuraccy:',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddcfdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.suptitle('Optimizer : Adam', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ebba23",
   "metadata": {},
   "source": [
    "## Second model in cnn i.e. to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second model\n",
    "# To prevent overfitting\n",
    "num_classes = 8\n",
    "model = Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
    "  layers.Dense(10, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(num_classes,kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.Dropout(0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2efe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling after overfitting prevention\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=test_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbefc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc=model.evaluate(test_ds,verbose=2)\n",
    "print('\\nTest accuraccy:',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84087f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.suptitle('Optimizer : Adam', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be536bd",
   "metadata": {},
   "source": [
    "## model 3 on cnn  added 4 layers and epochs to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd5b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding more layers and also batch normalization so that neural networks get faster and add hidden layers for better performance\n",
    "\n",
    "\n",
    "model=models.Sequential()\n",
    "\n",
    "#adding first cnn layer\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#adding second cnn layer\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#adding third cnn layer\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "#adding fourth cnn layer\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.40))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(8,activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3eb65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454981bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increasing epochs i.e. hyperparameters tuning\n",
    "epochs=50\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=test_ds, \n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc=model.evaluate(test_ds,verbose=2)\n",
    "print('\\nTest accuraccy:',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.suptitle('Optimizer : Adam', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef37696",
   "metadata": {},
   "source": [
    "## fourth model using cnn added 2 more layers and uses batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b48afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding more Layers and also batch normalization \n",
    "\n",
    "model=models.Sequential()\n",
    "\n",
    "#adding first cnn Layer \n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_regularizer=regularizers.12(0  model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) model.add(Dropout(0.25))\n",
    "\n",
    "#adding second cnn Layer \n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_regularizer=regularizers.12(0  model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Dropout(0.25))\n",
    "\n",
    "#adding third cnn Layer \n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_regularizer=regularizers.12(1  model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Dropout(0.30))\n",
    "\n",
    "#adding fourth cnn Layer \n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_regularizer=regularizers.12(1  model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Dropout(0.40))\n",
    "\n",
    "#adding fifth cnn Layer\n",
    "model.add(Conv2D(512,(3,3) , padding='same', kernel_regularizer=regularizers.. model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) model.add(Dropout(0.50))\n",
    "\n",
    "#adding sixth cnn Layer\n",
    "model.add(Conv2D(1024,(3,3) , padding='same\", kernel_regularizer=regularizers model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) model.add(Dropout(0.50))\n",
    "\n",
    "model.add(Flatten()) model.add(Dense(8,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f70e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increasing epochs i.e. hyperparameters tuning\n",
    "epochs=50\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=test_ds, \n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee9e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc=model.evaluate(test_ds,verbose=2)\n",
    "print('\\nTest accuraccy:',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.suptitle('Optimizer : Adam', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc851a",
   "metadata": {},
   "source": [
    "### plotting graphs for cnn models which has variations in each new model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run prediction on a sample image\n",
    "import numpy as np\n",
    "for images_batch, labels_batch in test_ds.take(1):\n",
    "    \n",
    "    first_image = images_batch[0].numpy().astype('uint8')\n",
    "    first_label = labels_batch[0].numpy()\n",
    "    \n",
    "    print(\"first image to predict\")\n",
    "    plt.imshow(first_image)\n",
    "    print(\"actual label:\",class_names[first_label])\n",
    "    \n",
    "    batch_prediction = model.predict(images_batch)\n",
    "    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a96d75",
   "metadata": {},
   "source": [
    "## Predicting the image using the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f6714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for prediction\n",
    "def predict(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbcddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now run function on few sample images\n",
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        \n",
    "        predicted_class, confidence = predict(model, images[i].numpy())\n",
    "        actual_class = class_names[labels[i]] \n",
    "        \n",
    "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "        \n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5852c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./leaf_diseases.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c659873",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62802d12",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab5f09",
   "metadata": {},
   "source": [
    "  ### Trying to classify the images using other ML models such as Random forest, KNN, Decision tree, Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Set the path to the dataset directory\n",
    "dataset_dir = \"./leaf_diseases\"\n",
    "\n",
    "# Define the image dimensions\n",
    "img_width = 256\n",
    "img_height = 256\n",
    "\n",
    "# Initialize the data and label arrays\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop over each subdirectory in the dataset directory\n",
    "for class_dir in os.listdir(dataset_dir):\n",
    "    class_path = os.path.join(dataset_dir, class_dir)\n",
    "    \n",
    "    # Loop over each image in the class directory\n",
    "    for image_name in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "        \n",
    "        # Load the image and resize it to the desired dimensions\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (img_width, img_height))\n",
    "        \n",
    "        # Append the image and class label to the data and label arrays\n",
    "        X.append(image)\n",
    "        y.append(class_dir)\n",
    "\n",
    "# Convert the data and label arrays to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Initialize the classifiers\n",
    "rf_clf = RandomForestClassifier()\n",
    "knn_clf = KNeighborsClassifier()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "nb_clf = GaussianNB()\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Initialize the KFold object\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "# Loop over each fold\n",
    "for fold_idx, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "    print(\"Fold: \", fold_idx+1)\n",
    "    \n",
    "    # Split the data and label arrays into training and testing subsets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Reshape the data arrays to 1D arrays\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    # Train and test the Random Forest classifier\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    rf_acc = rf_clf.score(X_test, y_test)\n",
    "    print(\"Random Forest accuracy: \", rf_acc)\n",
    "    \n",
    "    # Train and test the KNN classifier\n",
    "    knn_clf.fit(X_train, y_train)\n",
    "    knn_acc = knn_clf.score(X_test, y_test)\n",
    "    print(\"KNN accuracy: \", knn_acc)\n",
    "    \n",
    "    # Train and test the Decision Tree classifier\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    dt_acc = dt_clf.score(X_test, y_test)\n",
    "    print(\"Decision Tree accuracy: \", dt_acc)\n",
    "    \n",
    "    # Train and test the Naive Bayes classifier\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    nb_acc = nb_clf.score(X_test, y_test)\n",
    "    print(\"Naive Bayes accuracy: \", nb_acc)\n",
    "    \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960dc844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the graph for all  other ML models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the classifiers and their corresponding colors\n",
    "classifiers = [\"Random Forest\", \"KNN\", \"Decision Tree\", \"Naive Bayes\"]\n",
    "colors = [\"red\", \"green\", \"blue\", \"orange\"]\n",
    "\n",
    "# Initialize the accuracy array for each classifier\n",
    "rf_accs = []\n",
    "knn_accs = []\n",
    "dt_accs = []\n",
    "nb_accs = []\n",
    "\n",
    "# Loop over each fold\n",
    "for fold_idx, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "    \n",
    "    # Split the data and label arrays into training and testing subsets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Reshape the data arrays to 1D arrays\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    # Train and test the classifiers\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    rf_acc = rf_clf.score(X_test, y_test)\n",
    "    rf_accs.append(rf_acc)\n",
    "    \n",
    "    knn_clf.fit(X_train, y_train)\n",
    "    knn_acc = knn_clf.score(X_test, y_test)\n",
    "    knn_accs.append(knn_acc)\n",
    "    \n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    dt_acc = dt_clf.score(X_test, y_test)\n",
    "    dt_accs.append(dt_acc)\n",
    "    \n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    nb_acc = nb_clf.score(X_test, y_test)\n",
    "    nb_accs.append(nb_acc)\n",
    "\n",
    "# Initialize the figure and axis objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the bar chart for each classifier\n",
    "ax.bar([0, 1, 2, 3], [np.mean(rf_accs), np.mean(knn_accs), np.mean(dt_accs), np.mean(nb_accs)], color=colors)\n",
    "\n",
    "# Add the labels to the x-axis and y-axis\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(classifiers)\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45415924",
   "metadata": {},
   "source": [
    "## Final graph for all models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1367a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Data\n",
    "accuracies = [0.9449, 0.87, 0.675, 0.69, 0.5425]\n",
    "classifiers = ['CNN', 'Random Forest', 'KNN', 'Decision Tree', 'Naive Bayes']\n",
    " \n",
    "colors = ['blue', 'green', 'purple', 'orange', 'red']\n",
    " \n",
    "# Creating axes instance\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    " \n",
    "# Creating bar plot\n",
    "ax.bar(classifiers, accuracies, color=colors)\n",
    " \n",
    "# Set title and labels for axes\n",
    "ax.set_title('Accuracy of different classifiers')\n",
    "ax.set_xlabel('Classifiers')\n",
    "ax.set_ylabel('Accuracy')\n",
    " \n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
